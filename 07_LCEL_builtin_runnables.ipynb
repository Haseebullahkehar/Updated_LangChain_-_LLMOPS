{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.RunnablePassThrough\n",
    "\n",
    "2.RunnableLambda\n",
    "\n",
    "3.Runnable parallel \n",
    "\n",
    "      - itemgetter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI \n",
    "\n",
    "model = ChatOpenAI(model = \"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunnablePassthrough**\n",
    "\n",
    "- it does not do anything to the input data.\n",
    "- Let's see it in a very simple example: a chain with just RunnablePassthrough() will output the original without any modification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = RunnablePassthrough()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bappy'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Bappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RunnableLambda**\n",
    "- To use a custom function inside a LCEL chain we need to wrap it up with RunnableLambda.\n",
    "- Let's define a very function to create Russian lastnames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname(name: str) -> str:\n",
    "    return f\"{name}ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hassiiovich'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "russian_lastname(\"Hassii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "chain = RunnablePassthrough() | RunnableLambda(russian_lastname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hassiiovich'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"Hassii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RunnableParallel \n",
    "- We will use RunnableParallel() for running tasks in parallel.\n",
    "- This is probably the most important and most useful Runnable from Langchain.\n",
    "- In the following chain, RunnableParallel is going to run these two tasks in parallel:\n",
    "       - operation_a will use RunnablePassthrough.\n",
    "       - operation_b will use RunnableLambda with the russian_lastname function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel \n",
    "\n",
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"operation_b\": RunnableLambda(russian_lastname)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'operation_a': 'HASSII', 'operation_b': 'HASSIIovich'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke(\"HASSII\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"tell me a curious fact about {soccer_player}\")\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def russian_lastname_from_dictionary(person):\n",
    "    return person['name'] + \"ovich\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = RunnableParallel(\n",
    "    {\n",
    "        \"operation_a\": RunnablePassthrough(),\n",
    "        \"soccer_player\": RunnableLambda(russian_lastname_from_dictionary),\n",
    "        \"operation_b\": RunnablePassthrough(),\n",
    "    }\n",
    ")  | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One interesting fact about Roman Abramovich is that he is known for his extravagant lifestyle and love for luxury. He is the owner of several luxury properties, yachts, and private jets, and has been known to host lavish parties and events. Additionally, he is a prominent art collector and has one of the largest private art collections in the world. Abramovich is also known for his philanthropy, having donated millions of dollars to various charitable causes and organizations.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\n",
    "              \"name1\": \"Jordam\",\n",
    "              \"name\": \"Abrahm\"\n",
    "              })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's see a more advanced use of RunnableParallel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dswithbappy is a platform that focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python Programming, etc in English.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
    "\n",
    "# Define the embedding model\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Create the vectorstore with the embedding model\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python Programming, etc in English\"],\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "retrieval_chain = (\n",
    "    RunnableParallel({\"context\": retriever, \"question\": RunnablePassthrough()})\n",
    "    | prompt   \n",
    "    | model\n",
    "    | StrOutputParser()\n",
    "    )   \n",
    "\n",
    "retrieval_chain.invoke(\"what is dswithbappy?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Arrr, dswithbappy be a scallywag that focuses on providin' content on Data Science, AI, ML, DL, CV, NLP, Python Programmin', etc in English. Aye, me hearties!\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser \n",
    "from langchain.vectorstores import FAISS \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel \n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings \n",
    "from operator import itemgetter\n",
    "\n",
    "# Define the embedding model\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")\n",
    "\n",
    "\n",
    "# Create the vectorstore with the embedding model\n",
    "\n",
    "vector_db = FAISS.from_texts(\n",
    "    [\"dswithbappy focuses on providing content on Data Science, AI, ML, DL, CV, NLP, Python Programming, etc in English\"],\n",
    "    embedding_model\n",
    ")\n",
    "\n",
    "\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Language: {language}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"language\": itemgetter(\"language\"),\n",
    "    }\n",
    "    | prompt \n",
    "    | model \n",
    "    | StrOutputParser()\n",
    "\n",
    ")\n",
    "\n",
    "chain.invoke({\"question\": \"what is dswithbappy?\", \"language\": \"Pirate English\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LC_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
