{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "chatModel = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CodeBasics is a popular YouTube channel and website that provides tutorials and courses on programming, data science, machine learning, and other technical topics. The creator of CodeBasics, a software developer named Dhaval Patel, offers a wide range of educational content for beginners and advanced learners alike. The tutorials are known for being clear, concise, and easy to follow, making complex technical concepts more accessible to a broad audience. CodeBasics is a valuable resource for individuals looking to enhance their coding skills or pursue a career in technology.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    (\"system\", \"You are an helpful assistant.\"),\n",
    "    (\"human\", \"tell me about CodeBasics\"),\n",
    "]\n",
    "response = chatModel.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf data loading\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data/be-good.txt\")\n",
    "loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'data/be-good.txt'}, page_content='This essay is derived from a talk at the 2008 Startup School.)About a month after we started Y Combinator we came up with the\\\\nphrase that became our motto: Make something people want.  We\\\\\\'ve\\\\nlearned a lot since then, but if I were choosing now that\\\\\\'s still\\\\nthe one I\\\\\\'d pick.Another thing we tell founders is not to worry too much about the\\\\nbusiness model, at least at first.  Not because making money is\\\\nunimportant, but because it\\\\\\'s so much easier than building something\\\\ngreat.A couple weeks ago I realized that if you put those two ideas\\\\ntogether, you get something surprising.  Make something people want.\\\\nDon\\\\\\'t worry too much about making money.  What you\\\\\\'ve got is a\\\\ndescription of a charity.When you get an unexpected result like this, it could either be a\\\\nbug or a new discovery.  Either businesses aren\\\\\\'t supposed to be\\\\nlike charities, and we\\\\\\'ve proven by reductio ad absurdum that one\\\\nor both of the principles we began with is false.  Or we have a new\\\\nidea.I suspect it\\\\\\'s the latter, because as soon as this thought occurred\\\\nto me, a whole bunch of other things fell into place.ExamplesFor example, Craigslist.  It\\\\\\'s not a charity, but they run it like\\\\none.  And they\\\\\\'re astoundingly successful.  When you scan down the\\\\nlist of most popular web sites, the number of employees at Craigslist\\\\nlooks like a misprint. Their revenues aren\\\\\\'t as high as they could\\\\nbe, but most startups would be happy to trade places with them.In Patrick O\\\\\\'Brian\\\\\\'s novels, his captains always try to get upwind\\\\nof their opponents.  If you\\\\\\'re upwind, you decide when and if to\\\\nengage the other ship.  Craigslist is effectively upwind of enormous\\\\nrevenues.  They\\\\\\'d face some challenges if they wanted to make more,\\\\nbut not the sort you face when you\\\\\\'re tacking upwind, trying to\\\\nforce a crappy product on ambivalent users by spending ten times\\\\nas much on sales as on development.  [1]I\\\\\\'m not saying startups should aim to end up like Craigslist.\\\\nThey\\\\\\'re a product of unusual circumstances.  But they\\\\\\'re a good\\\\nmodel for the early phases.Google looked a lot like a charity in the beginning. They didn\\\\\\'t\\\\nhave ads for over a year.  At year 1, Google was indistinguishable\\\\nfrom a nonprofit.  If a nonprofit or government organization had\\\\nstarted a project to index the web, Google at year 1 is the limit\\\\nof what they\\\\\\'d have produced.Back when I was working on spam filters I thought it would be a\\\\ngood idea to have a web-based email service with good spam filtering.\\\\nI wasn\\\\\\'t thinking of it as a company.  I just wanted to keep people\\\\nfrom getting spammed.  But as I thought more about this project, I\\\\nrealized it would probably have to be a company.  It would cost\\\\nsomething to run, and it would be a pain to fund with grants and\\\\ndonations.That was a surprising realization.  Companies often claim to be\\\\nbenevolent, but it was surprising to realize there were purely\\\\nbenevolent projects that had to be embodied as companies to work.I didn\\\\\\'t want to start another company, so I didn\\\\\\'t do it.  But if\\\\nsomeone had, they\\\\\\'d probably be quite rich now.  There was a window\\\\nof about two years when spam was increasing rapidly but all the big\\\\nemail services had terrible filters.  If someone had launched a\\\\nnew, spam-free mail service, users would have flocked to it.Notice the pattern here?  From either direction we get to the same\\\\nspot.  If you start from successful startups, you find they often\\\\nbehaved like nonprofits.  And if you start from ideas for nonprofits,\\\\nyou find they\\\\\\'d often make good startups.PowerHow wide is this territory?  Would all good nonprofits be good\\\\ncompanies?  Possibly not.  What makes Google so valuable is that\\\\ntheir users have money.  If you make people with money love you,\\\\nyou can probably get some of it.  But could you also base a successful\\\\nstartup on behaving like a nonprofit to people who don\\\\\\'t have money?\\\\nCould you, for example, grow a successful startup out of curing an\\\\nunfashionable but deadly disease like malaria?I\\\\\\'m not sure, but I suspect that if you pushed this idea, you\\\\\\'d be\\\\nsurprised how far it would go.  For example, people who apply to Y\\\\nCombinator don\\\\\\'t generally have much money, and yet we can profit\\\\nby helping them, because with our help they could make money.  Maybe\\\\nthe situation is similar with malaria.  Maybe an organization that\\\\nhelped lift its weight off a country could benefit from the resulting\\\\ngrowth.I\\\\\\'m not proposing this is a serious idea.  I don\\\\\\'t know anything\\\\nabout malaria.  But I\\\\\\'ve been kicking ideas around long enough to\\\\nknow when I come across a powerful one.One way to guess how far an idea extends is to ask yourself at what\\\\npoint you\\\\\\'d bet against it.  The thought of betting against benevolence\\\\nis alarming in the same way as saying that something is technically\\\\nimpossible.  You\\\\\\'re just asking to be made a fool of, because these\\\\nare such powerful forces.  [2]For example, initially I thought maybe this principle only applied\\\\nto Internet startups.  Obviously it worked for Google, but what\\\\nabout Microsoft?  Surely Microsoft isn\\\\\\'t benevolent?  But when I\\\\nthink back to the beginning, they were.  Compared to IBM they were\\\\nlike Robin Hood.  When IBM introduced the PC, they thought they\\\\nwere going to make money selling hardware at high prices.  But by\\\\ngaining control of the PC standard, Microsoft opened up the market\\\\nto any manufacturer.  Hardware prices plummeted, and lots of people\\\\ngot to have computers who couldn\\\\\\'t otherwise have afforded them.\\\\nIt\\\\\\'s the sort of thing you\\\\\\'d expect Google to do.Microsoft isn\\\\\\'t so benevolent now.  Now when one thinks of what\\\\nMicrosoft does to users, all the verbs that come to mind begin with\\\\nF.  [3] And yet it doesn\\\\\\'t seem to pay.\\\\nTheir stock price has been flat for years.  Back when they were\\\\nRobin Hood, their stock price rose like Google\\\\\\'s.  Could there be\\\\na connection?You can see how there would be.  When you\\\\\\'re small, you can\\\\\\'t bully\\\\ncustomers, so you have to charm them.  Whereas when you\\\\\\'re big you\\\\ncan maltreat them at will, and you tend to, because it\\\\\\'s easier\\\\nthan satisfying them.  You grow big by being nice, but you can stay\\\\nbig by being mean.You get away with it till the underlying conditions change, and\\\\nthen all your victims escape.  So \"Don\\\\\\'t be evil\" may be the most\\\\nvaluable thing Paul Buchheit made for Google, because it may turn\\\\nout to be an elixir of corporate youth.  I\\\\\\'m sure they find it\\\\nconstraining, but think how valuable it will be if it saves them\\\\nfrom lapsing into the fatal laziness that afflicted Microsoft and\\\\nIBM.The curious thing is, this elixir is freely available to any other\\\\ncompany.  Anyone can adopt \"Don\\\\\\'t be evil.\"  The catch is that\\\\npeople will hold you to it.  So I don\\\\\\'t think you\\\\\\'re going to see\\\\nrecord labels or tobacco companies using this discovery.MoraleThere\\\\\\'s a lot of external evidence that benevolence works.  But how\\\\ndoes it work?  One advantage of investing in a large number of\\\\nstartups is that you get a lot of data about how they work.  From\\\\nwhat we\\\\\\'ve seen, being good seems to help startups in three ways:\\\\nit improves their morale, it makes other people want to help them,\\\\nand above all, it helps them be decisive.Morale is tremendously important to a startup—so important\\\\nthat morale alone is almost enough to determine success.  Startups\\\\nare often described as emotional roller-coasters. One minute you\\\\\\'re\\\\ngoing to take over the world, and the next you\\\\\\'re doomed.  The\\\\nproblem with feeling you\\\\\\'re doomed is not just that it makes you\\\\nunhappy, but that it makes you stop working.  So the downhills\\\\nof the roller-coaster are more of a self fulfilling prophecy than\\\\nthe uphills.  If feeling you\\\\\\'re going to succeed makes you work\\\\nharder, that probably improves your chances of succeeding, but if\\\\nfeeling you\\\\\\'re going to fail makes you stop working, that practically\\\\nguarantees you\\\\\\'ll fail.Here\\\\\\'s where benevolence comes in.  If you feel you\\\\\\'re really helping\\\\npeople, you\\\\\\'ll keep working even when it seems like your startup\\\\nis doomed.  Most of us have some amount of natural benevolence.\\\\nThe mere fact that someone needs you makes you want to help them.\\\\nSo if you start the kind of startup where users come back each day,\\\\nyou\\\\\\'ve basically built yourself a giant tamagotchi.  You\\\\\\'ve made\\\\nsomething you need to take care of.Blogger is a famous example of a startup that went through really\\\\nlow lows and survived.  At one point they ran out of money and\\\\neveryone left. Evan Williams came in to work the next day, and there\\\\nwas no one but him.  What kept him going?  Partly that users needed\\\\nhim.  He was hosting thousands of people\\\\\\'s blogs. He couldn\\\\\\'t just\\\\nlet the site die.There are many advantages of launching quickly, but the most important\\\\nmay be that once you have users, the tamagotchi effect kicks in.\\\\nOnce you have users to take care of, you\\\\\\'re forced to figure out\\\\nwhat will make them happy, and that\\\\\\'s actually very valuable\\\\ninformation.The added confidence that comes from trying to help people can\\\\nalso help you with investors. One of the founders of \\\\nChatterous told \\\\nme recently that he and his cofounder had decided that this service\\\\nwas something the world needed, so they were going to keep working\\\\non it no matter what, even if they had to move back to Canada and live\\\\nin their parents\\\\\\' basements.Once they realized this, they stopped caring so much what investors thought\\\\nabout them.  They still met with them, but they weren\\\\\\'t going to\\\\ndie if they didn\\\\\\'t get their money.  And you know what?  The investors\\\\ngot a lot more interested.  They could sense that the Chatterouses\\\\nwere going to do this startup with or without them.If you\\\\\\'re really committed and your startup is cheap to run, you\\\\nbecome very hard to kill.  And practically all startups, even the\\\\nmost successful, come close to death at some point.  So if doing\\\\ngood for people gives you a sense of mission that makes you harder\\\\nto kill, that alone more than compensates for whatever you lose by\\\\nnot choosing a more selfish project.HelpAnother advantage of being good is that it makes other people want\\\\nto help you.  This too seems to be an inborn trait in humans.One of the startups we\\\\\\'ve funded, Octopart, is currently locked in\\\\na classic battle of good versus evil.  They\\\\\\'re a search site for\\\\nindustrial components.  A lot of people need to search for components,\\\\nand before Octopart there was no good way to do it.  That, it turned\\\\nout, was no coincidence.Octopart built the right way to search for components.  Users like\\\\nit and they\\\\\\'ve been growing rapidly.  And yet for most of Octopart\\\\\\'s\\\\nlife, the biggest distributor, Digi-Key, has been trying to force\\\\nthem take their prices off the site.  Octopart is sending them\\\\ncustomers for free, and yet Digi-Key is trying to make that traffic\\\\nstop.  Why?  Because their current business model depends on\\\\novercharging people who have incomplete information about prices.\\\\nThey don\\\\\\'t want search to work.The Octoparts are the nicest guys in the world.  They dropped out\\\\nof the PhD program in physics at Berkeley to do this.  They just\\\\nwanted to fix a problem they encountered in their research.  Imagine\\\\nhow much time you could save the world\\\\\\'s engineers if they could\\\\ndo searches online.  So when I hear that a big, evil company is\\\\ntrying to stop them in order to keep search broken, it makes me\\\\nreally want to help them. It makes me spend more time on the Octoparts\\\\nthan I do with most of the other startups we\\\\\\'ve funded.  It just\\\\nmade me spend several minutes telling you how great they are.  Why?\\\\nBecause they\\\\\\'re good guys and they\\\\\\'re trying to help the world.If you\\\\\\'re benevolent, people will rally around you: investors,\\\\ncustomers, other companies, and potential employees.  In the long\\\\nterm the most important may be the potential employees.  I think\\\\neveryone knows now that \\\\ngood hackers are much better than mediocre\\\\nones.  If you can attract the best hackers to work for you, as\\\\nGoogle has, you have a big advantage.  And the very best hackers\\\\ntend to be idealistic.  They\\\\\\'re not desperate for a job.  They can\\\\nwork wherever they want.  So most want to work on things that will\\\\nmake the world better.CompassBut the most important advantage of being good is that it acts as\\\\na compass.  One of the hardest parts of doing a startup is that you\\\\nhave so many choices.  There are just two or three of you, and a\\\\nthousand things you could do. How do you decide?Here\\\\\\'s the answer: Do whatever\\\\\\'s best for your users.  You can hold\\\\nonto this like a rope in a hurricane, and it will save you if\\\\nanything can.  Follow it and it will take you through everything\\\\nyou need to do.It\\\\\\'s even the answer to questions that seem unrelated, like how to\\\\nconvince investors to give you money.  If you\\\\\\'re a good salesman,\\\\nyou could try to just talk them into it.  But the more reliable\\\\nroute is to convince them through your users: if you make something\\\\nusers love enough to tell their friends, you grow exponentially,\\\\nand that will convince any investor.Being good is a particularly useful strategy for making decisions\\\\nin complex situations because it\\\\\\'s stateless.  It\\\\\\'s like telling\\\\nthe truth.  The trouble with lying is that you have to remember\\\\neverything you\\\\\\'ve said in the past to make sure you don\\\\\\'t contradict\\\\nyourself.  If you tell the truth you don\\\\\\'t have to remember anything,\\\\nand that\\\\\\'s a really useful property in domains where things happen\\\\nfast.For example, Y Combinator has now invested in 80 startups, 57 of\\\\nwhich are still alive.  (The rest have died or merged or been\\\\nacquired.)  When you\\\\\\'re trying to advise 57 startups, it turns out\\\\nyou have to have a stateless algorithm.  You can\\\\\\'t have ulterior\\\\nmotives when you have 57 things going on at once, because you can\\\\\\'t\\\\nremember them.  So our rule is just to do whatever\\\\\\'s best for the\\\\nfounders.  Not because we\\\\\\'re particularly benevolent, but because\\\\nit\\\\\\'s the only algorithm that works on that scale.When you write something telling people to be good, you seem to be\\\\nclaiming to be good yourself.  So I want to say explicitly that I\\\\nam not a particularly good person.  When I was a kid I was firmly\\\\nin the camp of bad.  The way adults used the word good, it seemed\\\\nto be synonymous with quiet, so I grew up very suspicious of it.You know how there are some people whose names come up in conversation\\\\nand everyone says \"He\\\\\\'s such a great guy?\"  People never say\\\\nthat about me.  The best I get is \"he means well.\"  I am not claiming\\\\nto be good.  At best I speak good as a second language.So I\\\\\\'m not suggesting you be good in the usual sanctimonious way.\\\\nI\\\\\\'m suggesting it because it works.  It will work not just as a\\\\nstatement of \"values,\" but as a guide to strategy,\\\\nand even a design spec for software.  Don\\\\\\'t just not be evil.  Be\\\\ngood.Notes[1] Fifty years ago\\\\nit would have seemed shocking for a public company not to pay\\\\ndividends.  Now many tech companies don\\\\\\'t.  The markets seem to\\\\nhave figured out how to value potential dividends.  Maybe that isn\\\\\\'t\\\\nthe last step in this evolution.  Maybe markets will eventually get\\\\ncomfortable with potential earnings. (VCs already are, and at least\\\\nsome of them consistently make money.)I realize this sounds like the stuff one used to hear about the\\\\n\"new economy\" during the Bubble.  Believe me, I was not drinking\\\\nthat kool-aid at the time.  But I\\\\\\'m convinced there were some \\\\ngood\\\\nideas buried in Bubble thinking.  For example, it\\\\\\'s ok to focus on\\\\ngrowth instead of profits—but only if the growth is genuine.\\\\nYou can\\\\\\'t be buying users; that\\\\\\'s a pyramid scheme.   But a company\\\\nwith rapid, genuine growth is valuable, and eventually markets learn\\\\nhow to value valuable things.[2] The idea of starting\\\\na company with benevolent aims is currently undervalued, because\\\\nthe kind of people who currently make that their explicit goal don\\\\\\'t\\\\nusually do a very good job.It\\\\\\'s one of the standard career paths of trustafarians to start\\\\nsome vaguely benevolent business.  The problem with most of them\\\\nis that they either have a bogus political agenda or are feebly\\\\nexecuted.  The trustafarians\\\\\\' ancestors didn\\\\\\'t get rich by preserving\\\\ntheir traditional culture; maybe people in Bolivia don\\\\\\'t want to\\\\neither.  And starting an organic farm, though it\\\\\\'s at least\\\\nstraightforwardly benevolent, doesn\\\\\\'t help people on the scale that\\\\nGoogle does.Most explicitly benevolent projects don\\\\\\'t hold themselves sufficiently\\\\naccountable.  They act as if having good intentions were enough to\\\\nguarantee good effects.[3] Users dislike their\\\\nnew operating system so much that they\\\\\\'re starting petitions to\\\\nsave the old one.  And the old one was nothing special.  The hackers\\\\nwithin Microsoft must know in their hearts that if the company\\\\nreally cared about users they\\\\\\'d just advise them to switch to OSX.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston,\\\\nand Robert Morris for reading drafts of this.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CSV loader\n",
    "\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "\n",
    "loader = CSVLoader('data/Dataset .csv')\n",
    "\n",
    "loaded_data = loader.load()\n",
    "# loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_community.document_loaders import UnstructuredHTMLLoader\n",
    "\n",
    "# loader = UnstructuredHTMLLoader('./data/100-startups.html')\n",
    "\n",
    "# loaded_data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pdf data\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader('data/National AI Policy Consultation Draft V1.pdf')\n",
    "\n",
    "loaded_data_pdf = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaded_data_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wikipedia data loader\n",
    "\n",
    "from langchain_community.document_loaders import WikipediaLoader\n",
    "\n",
    "loader = WikipediaLoader(query=\"AI Policy\", load_max_docs=1)\n",
    "\n",
    "loaded_data = loader.load()[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The ethics of artificial intelligence covers a broad range of topics within the field that are considered to have particular ethical stakes. This includes algorithmic biases, fairness, automated decision-making, accountability, privacy, and regulation. \\nIt also covers various emerging or potential future challenges such as machine ethics (how to make machines that behave ethically), lethal autonomous weapon systems, arms race dynamics, AI safety and alignment, technological unemployment, AI-enabled misinformation, how to treat certain AI systems if they have a moral status (AI welfare and rights), artificial superintelligence and existential risks. \\nSome application areas may also have particularly important ethical implications, like healthcare, education, criminal justice, or the military.\\n\\n\\n== Machine ethics ==\\n\\nMachine ethics (or machine morality) is the field of research concerned with designing Artificial Moral Agents (AMAs), robots or artificially intelligent computers that behave morally or as though moral. To account for the nature of these agents, it has been suggested to consider certain philosophical ideas, like the standard characterizations of agency, rational agency, moral agency, and artificial agency, which are related to the concept of AMAs.\\nThere are discussions on creating tests to see if an AI is capable of making ethical decisions. Alan Winfield concludes that the Turing test is flawed and the requirement for an AI to pass the test is too low. A proposed alternative test is one called the Ethical Turing Test, which would improve on the current test by having multiple judges decide if the AI\\'s decision is ethical or unethical. Neuromorphic AI could be one way to create morally capable robots, as it aims to process information similarly to humans, nonlinearly and with millions of interconnected artificial neurons. Similarly, whole-brain emulation (scanning a brain and simulating it on digital hardware) could also in principle lead to human-like robots, thus capable of moral actions. And large language models are capable of approximating human moral judgments. Inevitably, this raises the question of the environment in which such robots would learn about the world and whose morality they would inherit – or if they end up developing human \\'weaknesses\\' as well: selfishness, pro-survival attitudes, inconsistency, scale insensitivity, etc.\\nIn Moral Machines: Teaching Robots Right from Wrong, Wendell Wallach and Colin Allen conclude that attempts to teach robots right from wrong will likely advance understanding of human ethics by motivating humans to address gaps in modern normative theory and by providing a platform for experimental investigation. As one example, it has introduced normative ethicists to the controversial issue of which specific learning algorithms to use in machines. For simple decisions, Nick Bostrom and Eliezer Yudkowsky have argued that decision trees (such as ID3) are more transparent than neural networks and genetic algorithms, while Chris Santos-Lang argued in favor of machine learning on the grounds that the norms of any age must be allowed to change and that natural failure to fully satisfy these particular norms has been essential in making humans less vulnerable to criminal \"hackers\".\\n\\n\\n=== Robot ethics ===\\n\\nThe term \"robot ethics\" (sometimes \"roboethics\") refers to the morality of how humans design, construct, use and treat robots. Robot ethics intersect with the ethics of AI. Robots are physical machines whereas AI can be only software. Not all robots function through AI systems and not all AI systems are robots. Robot ethics considers how machines may be used to harm or benefit humans, their impact on individual autonomy, and their effects on social justice.\\n\\n\\n=== Ethical principles ===\\nIn the review of 84 ethics guidelines for AI, 11 clusters of principles were found: transparency, justice and fairness, non-maleficence, responsibility, privacy, beneficence, freedom and autonomy, tr'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate \n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"Answer this {question}, here is some extra {context}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "messages = chat_template.format_messages(\n",
    "    name=\"AI Policy\",\n",
    "    question=\"Tell me about AI Policy\",\n",
    "    context = loaded_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chatModel.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ust, dignity, sustainability, and solidarity. These principles are meant to guide the development and deployment of AI systems in a way that aligns with ethical values and societal norms. \\n\\nTransparency refers to the need for AI systems to be explainable and understandable to users. Justice and fairness involve ensuring that AI systems do not discriminate against individuals or groups. Non-maleficence emphasizes the importance of preventing harm caused by AI systems. Responsibility involves assigning accountability for the actions of AI systems. Privacy focuses on protecting individuals' personal information. Beneficence emphasizes the need for AI systems to promote well-being. Freedom and autonomy highlight the importance of respecting individuals' choices and agency. Trust is essential for users to have confidence in AI systems. Dignity involves treating individuals with respect and consideration. Sustainability emphasizes the need for AI systems to consider long-term societal and environmental impacts. Solidarity involves working towards common goals and values in the development and deployment of AI systems. \\n\\nOverall, AI policy aims to address these ethical considerations and ensure that AI technologies are developed and used in a way that benefits society while minimizing potential harms.\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LC_series",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
